{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch vs Stream\n",
    "# Here are some differences between batch processing and stream processing: \n",
    "\n",
    "# Data size\n",
    "# In batch processing, the data size is known and finite. In stream processing, the data size is unknown and infinite.\n",
    "\n",
    "# Data collection\n",
    "# In batch processing, data is collected over time and then processed all at once. In stream processing, data is continuously collected and processed in real time as it arrives.\n",
    "\n",
    "# Data processing\n",
    "# Batch processing handles a large batch of data. Stream processing handles individual records or micro batches of few records.\n",
    "\n",
    "# Use cases\n",
    "# Batch processing is generally meant for large data quantities that are not time sensitive. Stream processing is typically meant for data needed immediately. Stream processing applications can handle high volumes and velocities of data, and provide low-latency and timely insights. Stream processing is suitable for use cases such as fraud detection, anomaly detection, event processing, and real-time analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK was designed for batch processing but can do stream procssing\n",
    "#Flink is true stream processing framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evolution of big data framework\n",
    "\n",
    "# MapReduce (2004): Google introduced the concept of MapReduce as a programming model for processing and generating large datasets on a distributed cluster. This inspired open-source implementations like Apache Hadoop.\n",
    "\n",
    "# Hadoop (2006): The Apache Hadoop project emerged as an open-source implementation of Google's MapReduce, providing a distributed file system (HDFS) and a framework for processing large-scale data across clusters. Hadoop played a pivotal role in making big data accessible to a broader audience.\n",
    "\n",
    "# Apache Spark (2010): Spark was introduced as an open-source, distributed computing system that aimed to improve upon the limitations of MapReduce. It introduced in-memory processing, which significantly accelerated iterative algorithms and machine learning workflows compared to Hadoop.\n",
    "\n",
    "# Apache Flink (2014): Flink is a stream processing framework that focuses on low-latency, high-throughput processing of continuous data streams. It's designed to handle both batch and stream processing, offering better performance and expressiveness for certain use cases compared to Spark.\n",
    "\n",
    "# Apache Kafka (2011): Kafka is a distributed streaming platform that enables the building of real-time data pipelines and streaming applications. It's widely used for its durability, fault-tolerance, and ability to handle high-throughput data streams.\n",
    "\n",
    "# Apache Storm (2011): Storm was one of the early stream processing frameworks, designed for real-time data processing. While it has been succeeded by other frameworks like Flink and Spark Streaming for some use cases, Storm played a role in shaping the landscape of real-time data processing.\n",
    "\n",
    "# Apache HBase (2007): HBase is a distributed, scalable, and consistent NoSQL database that runs on top of Hadoop's HDFS. It provides random access to large amounts of structured data and is often used for real-time read and write access to big data.\n",
    "\n",
    "# Apache Beam (2016): Beam is a unified stream and batch processing model designed to provide a portable, flexible, and extensible way to express data processing pipelines. It supports multiple backends, including Apache Spark and Apache Flink.\n",
    "\n",
    "# Dask (2015): Dask is a parallel computing library that integrates with Python and allows users to harness the full power of their CPU and memory resources for analytics. While not exclusively a big data framework, it is often used for scalable data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above part is google implementation of mapreduce- not open sourced\n",
    "# Below part is open source implemntation \n",
    "# Apache Hadoop was created to delegate data processing to several servers instead of running the workload on a single machine.\n",
    "\n",
    "# Meanwhile, Apache Spark is a newer data processing system that overcomes key limitations of Hadoop(memory). Despite its ability to process large datasets, Hadoop only does so in batches and with substantial delay.\n",
    "\n",
    "#Flume is equivalent of flink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Google big data ecosystem\n",
    "# Dataflow is on appache beam\n",
    "# Dataproc is on haddop spark\n",
    "# Dataprep is for cleaning data \n",
    "# We load data from stroge to dataprep to clean before giving it to data flow or dataprac\n",
    "\n",
    "#Ingestion layer for storing data in input data layer from external"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark vs Flink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Architecture\n",
    "# Apache Beam is an open-source, unified model for defining both batch and streaming data-parallel processing pipelines. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Apex, Apache Flink, Apache Spark, and Google Cloud Dataflow. —"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fow of beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache-beam\n",
      "  Downloading apache_beam-2.52.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting crcmod<2.0,>=1.7 (from apache-beam)\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m309.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: orjson<4,>=3.9.7 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (3.9.10)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (0.3.1.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (2.2.1)\n",
      "Collecting fastavro<2,>=0.23.6 (from apache-beam)\n",
      "  Downloading fastavro-1.9.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (5.5 kB)\n",
      "Collecting fasteners<1.0,>=0.3 (from apache-beam)\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (1.60.0)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam)\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httplib2<0.23.0,>=0.8 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (0.22.0)\n",
      "Collecting js2py<1,>=0.74 (from apache-beam)\n",
      "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (4.20.0)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (1.23.5)\n",
      "Collecting objsize<0.7.0,>=0.6.1 (from apache-beam)\n",
      "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (23.2)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (4.4.1)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (4.25.1)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (2023.3.post1)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (2023.10.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (4.9.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (0.19.0)\n",
      "Requirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from apache-beam) (11.0.0)\n",
      "Collecting pyarrow-hotfix<1 (from apache-beam)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from httplib2<0.23.0,>=0.8->apache-beam) (3.0.9)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from js2py<1,>=0.74->apache-beam) (4.3.1)\n",
      "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam)\n",
      "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (0.13.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from pymongo<5.0.0,>=3.8.0->apache-beam) (2.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2023.11.17)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from tzlocal>=1.2->js2py<1,>=0.74->apache-beam) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in /Users/rahulmahajan/anaconda3/lib/python3.11/site-packages (from pytz-deprecation-shim->tzlocal>=1.2->js2py<1,>=0.74->apache-beam) (2023.3)\n",
      "Downloading apache_beam-2.52.0-cp311-cp311-macosx_10_9_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastavro-1.9.2-cp311-cp311-macosx_10_9_universal2.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m931.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Building wheels for collected packages: crcmod, hdfs, pyjsparser, docopt\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp311-cp311-macosx_10_9_x86_64.whl size=22006 sha256=b6fa7ce65230884426b82f2251a57c10343a660e6c23f5cc8a69e68eaead3fa5\n",
      "  Stored in directory: /Users/rahulmahajan/Library/Caches/pip/wheels/23/94/7a/8cb7d14597e6395ce969933f01aed9ea8fa5f5b4d4c8a61e99\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=6797dcdde71a15587227af0d54457d80ecb947bd623df6c6ce6fc566396ff137\n",
      "  Stored in directory: /Users/rahulmahajan/Library/Caches/pip/wheels/b9/1d/dc/eb0833be25464c359903d356c4204721c6a672c26ff164cdc3\n",
      "  Building wheel for pyjsparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25982 sha256=801d4daad165e5d41218fc3ddc97604677f78e6fe0619eb81757c27f8c44ccc3\n",
      "  Stored in directory: /Users/rahulmahajan/Library/Caches/pip/wheels/a5/9a/30/1003e89ab4555b81840ca46d361bf184f1e6ad880cae3b62a9\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=dba04144deb1767741c6422ec4c8981e50867025ec930ca03f118ad821e51fef\n",
      "  Stored in directory: /Users/rahulmahajan/Library/Caches/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "Successfully built crcmod hdfs pyjsparser docopt\n",
      "Installing collected packages: pyjsparser, docopt, crcmod, pyarrow-hotfix, objsize, fasteners, fastavro, hdfs, js2py, apache-beam\n",
      "Successfully installed apache-beam-2.52.0 crcmod-1.7 docopt-0.6.2 fastavro-1.9.2 fasteners-0.19 hdfs-2.7.3 js2py-0.74 objsize-0.6.1 pyarrow-hotfix-0.6 pyjsparser-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install apache-beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
