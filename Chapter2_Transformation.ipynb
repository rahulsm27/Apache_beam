{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Major parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  p1 = beam.Pipeline()#pipeline method controls lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attendance_count(\n",
    "#     p1\n",
    "#      |\n",
    "\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Transforms supports below files\n",
    "#text avro parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python only support pubsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-20.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create transform to create own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x1262265d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = beam.Pipeline()\n",
    "\n",
    "lines =(\n",
    "    p2\n",
    "    |beam.Create([1,2,3,4,5,6,7])\n",
    "    |beam.io.WriteToText('data/outcreate1')\n",
    "\n",
    "\n",
    ")\n",
    "p2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Visualize the output\n",
    "!{('head -n 20 data/outcreate1-00000-of-00001')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-24.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pubsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marco', 31)\n",
      "('Rebekah', 31)\n",
      "('Itoe', 31)\n",
      "('Edouard', 31)\n",
      "('Kyle', 62)\n",
      "('Kumiko', 31)\n",
      "('Gaston', 31)\n",
      "('Ayumi', 30)\n"
     ]
    }
   ],
   "source": [
    "p1 = beam.Pipeline()\n",
    "def SplitRow(element):\n",
    "    return element.split(',')\n",
    "\n",
    "attendance_count=(\n",
    "    p1\n",
    "     |beam.io.ReadFromText(\"dept_data.txt\")\n",
    "     |beam.Map(SplitRow) #takes one element as input and output\n",
    "     |beam.Filter(lambda record : record[3] == 'Accounts') \n",
    "     |beam.Map(lambda record : (record[1],1)) # append 1 to each record and get the name and 1 as two columns\n",
    "     |beam.CombinePerKey(sum)\n",
    "     |beam.io.WriteToText('data/output_new')\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "p1.run()\n",
    "!{('head -n 20 data/output_new-00000-of-00001')}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map vs Flatmap\n",
    "# Flat map 1 to Many mapping -> multiple output possible\n",
    "# Map only 1 to 1 mapping -> single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marco', 31)\n",
      "('Rebekah', 31)\n",
      "('Itoe', 31)\n",
      "('Edouard', 31)\n",
      "('Kyle', 62)\n",
      "('Kumiko', 31)\n",
      "('Gaston', 31)\n",
      "('Ayumi', 30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#labelling\n",
    "#other ways of running pipeline\n",
    "\n",
    "def SplitRow(element):\n",
    "    return element.split(',')\n",
    "\n",
    "with beam.Pipeline() as p1:\n",
    "    attendance_count=(\n",
    "        p1\n",
    "        |'Read from file' >> beam.io.ReadFromText(\"dept_data.txt\")\n",
    "        |'Map' >> beam.Map(SplitRow) #takes one element as input and output\n",
    "        |beam.Filter(lambda record : record[3] == 'Accounts') \n",
    "        |beam.Map(lambda record : (record[1],1)) # append 1 to each record and get the name and 1 as two columns\n",
    "        |beam.CombinePerKey(sum)\n",
    "        |beam.io.WriteToText('data/output_new')\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "#p1.run()\n",
    "!{('head -n 20 data/output_new-00000-of-00001')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching and flattening\n",
    "# flattens combines two pcollection if they have same datatype and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pardo is generic case. Map and Flatmap are special cases of pardo transform\n",
    "\n",
    "# Create a class inherating DoFn class. DoFn implements distributed processing\n",
    "# We need to overwrite only the process method.\n",
    "\n",
    "#can also use lambda function. beam internally creates dofn lightweight objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class SplitRow(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "    # return type -> list\n",
    "    return  [element.split(',')]\n",
    "  \n",
    "\n",
    "class FilterAccountsEmployee(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "    if element[3] == 'Accounts':\n",
    "      return [element]  \n",
    "    \n",
    "class PairEmployees(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "    return [(element[3]+\",\"+element[1], 1)]    \n",
    "  \n",
    "class Counting(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "    # return type -> list\n",
    "    (key, values) = element           # [Marco, Accounts  [1,1,1,1....] , Rebekah, Accounts [1,1,1,1,....] ]\n",
    "    return [(key, sum(values))]\n",
    "     \n",
    "\n",
    "p1 = beam.Pipeline()\n",
    "\n",
    "attendance_count = (\n",
    "    \n",
    "   p1\n",
    "    |beam.io.ReadFromText('dept_data.txt')\n",
    "    \n",
    "    |beam.ParDo(SplitRow())\n",
    "   # | 'Compute WordLength' >> beam.ParDo(lambda element: [ element.split(',') ]) \n",
    "\n",
    "    |beam.ParDo(FilterAccountsEmployee())\n",
    "    |beam.ParDo(PairEmployees())\n",
    "    | 'Group ' >> beam.GroupByKey()\n",
    "    | 'Sum using ParDo' >> beam.ParDo(Counting())  \n",
    "    \n",
    "    |beam.io.WriteToText('data/output_new_final')\n",
    "  \n",
    ")\n",
    "\n",
    "p1.run()\n",
    "\n",
    "# Sample the first 20 results, remember there are no ordering guarantees.\n",
    "!{('head -n 20 data/output_new_final-00000-of-00001')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combiner of Beam\n",
    "\n",
    "# Mini reducer which does the reduce task locally to a mapper machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-29.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "p = beam.Pipeline()\n",
    "\n",
    "class AverageFn(beam.CombineFn):\n",
    "  \n",
    "  def create_accumulator(self):\n",
    "     return (0.0, 0)   # initialize (sum, count)\n",
    "\n",
    "  def add_input(self, sum_count, input):\n",
    "    (sum, count) = sum_count\n",
    "    return sum + input, count + 1\n",
    "\n",
    "  def merge_accumulators(self, accumulators):\n",
    "    \n",
    "    ind_sums, ind_counts = zip(*accumulators)       # zip - [(27, 3), (39, 3), (18, 2)]  -->   [(27,39,18), (3,3,2)]\n",
    "    return sum(ind_sums), sum(ind_counts)        # (84,8)\n",
    "\n",
    "  def extract_output(self, sum_count):    \n",
    "    \n",
    "    (sum, count) = sum_count    # combine globally using CombineFn\n",
    "    return sum / count if count else float('NaN')\n",
    "  \n",
    "\n",
    "small_sum = (\n",
    "           p \n",
    "            | beam.Create([15,5,7,7,9,23,13,5])\n",
    "            | \"Combine Globally\" >> beam.CombineGlobally(AverageFn()) \n",
    "            | 'Write results' >> beam.io.WriteToText('data/combine')\n",
    "          )\n",
    "p.run()\n",
    "\n",
    "# Sample the first 20 results, remember there are no ordering guarantees.\n",
    "!{'head -n 20 data/combine-00000-of-00001'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite Transform -> Modular approach. Combine common transform in a class. Use that class\n",
    "\n",
    "# Example Mytransform below class defines common transform applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class MyTransform(beam.PTransform):\n",
    "  \n",
    "  def expand(self, input_coll):\n",
    "    \n",
    "    a = ( \n",
    "        input_coll\n",
    "                       | 'Group and sum1' >> beam.CombinePerKey(sum)\n",
    "                       | 'count filter accounts' >> beam.Filter(filter_on_count)\n",
    "                       | 'Regular accounts employee' >> beam.Map(format_output)\n",
    "              \n",
    "    )\n",
    "    return a\n",
    "\n",
    "def SplitRow(element):\n",
    "    return element.split(',')\n",
    "  \n",
    "  \n",
    "def filter_on_count(element):\n",
    "  name, count = element\n",
    "  if count > 30:\n",
    "    return element\n",
    "  \n",
    "def format_output(element):\n",
    "  name, count = element\n",
    "  return ', '.join((name.encode('ascii'),str(count),'Regular employee'))\n",
    "\n",
    "p = beam.Pipeline()\n",
    "\n",
    "input_collection = ( \n",
    "                      p \n",
    "                      | \"Read from text file\" >> beam.io.ReadFromText('dept_data.txt')\n",
    "                      | \"Split rows\" >> beam.Map(SplitRow)\n",
    "                   )\n",
    "\n",
    "accounts_count = (\n",
    "                      input_collection\n",
    "                      | 'Get all Accounts dept persons' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
    "                      | 'Pair each accounts employee with 1' >> beam.Map(lambda record: (\"Accounts, \" +record[1], 1))\n",
    "                      | 'composite accoubts' >> MyTransform()\n",
    "                      | 'Write results for account' >> beam.io.WriteToText('data/Account')\n",
    "                 )\n",
    "\n",
    "hr_count = (\n",
    "                input_collection\n",
    "                | 'Get all HR dept persons' >> beam.Filter(lambda record: record[3] == 'HR')\n",
    "                | 'Pair each hr employee with 1' >> beam.Map(lambda record: (\"HR, \" +record[1], 1))\n",
    "                | 'composite HR' >> MyTransform()\n",
    "                | 'Write results for hr' >> beam.io.WriteToText('data/HR')\n",
    "           ) \n",
    "p.run()\n",
    "  \n",
    "# Sample the first 20 results, remember there are no ordering guarantees.\n",
    "!{('head -n 20 data/Account-00000-of-00001')}\n",
    "!{('head -n 20 data/HR-00000-of-00001')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CoGroupBy for joins -> Inner join two files\n",
    "# Should be key value pairs\n",
    "# retTuple converts to key value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def retTuple(element):\n",
    "  \n",
    "  thisTuple=element.split(',')\n",
    "  return (thisTuple[0],thisTuple[1:])\n",
    "                \n",
    "p1 = beam.Pipeline()\n",
    "\n",
    "# Apply a ParDo to the PCollection \"words\" to compute lengths for each word.\n",
    "dep_rows = ( \n",
    "                p1\n",
    "                | \"Reading File 1\" >> beam.io.ReadFromText('dept_data.txt')\n",
    "                | 'Pair each employee with key' >> beam.Map(retTuple)          # {149633CM : [Marco,10,Accounts,1-01-2019]}\n",
    "    \n",
    "               )\n",
    "\n",
    "\n",
    "loc_rows = ( \n",
    "                p1\n",
    "                | \"Reading File 2\" >> beam.io.ReadFromText('location.txt') \n",
    "                | 'Pair each loc with key' >> beam.Map(retTuple)                # {149633CM : [9876843261,New York]}\n",
    "               )\n",
    "\n",
    "\n",
    "results = ({'dep_data': dep_rows, 'loc_data': loc_rows} \n",
    "           \n",
    "           | beam.CoGroupByKey()\n",
    "           | 'Write results' >> beam.io.WriteToText('data/result')\n",
    "          )\n",
    "\n",
    "\n",
    "p1.run()\n",
    "\n",
    "!{('head -n 20 data/result-00000-of-00001')}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
