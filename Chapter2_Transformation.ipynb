{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Major parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  p1 = beam.Pipeline()#pipeline method controls lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attendance_count(\n",
    "#     p1\n",
    "#      |\n",
    "\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Transforms supports below files\n",
    "#text avro parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python only support pubsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-20.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create transform to create own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x1262265d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = beam.Pipeline()\n",
    "\n",
    "lines =(\n",
    "    p2\n",
    "    |beam.Create([1,2,3,4,5,6,7])\n",
    "    |beam.io.WriteToText('data/outcreate1')\n",
    "\n",
    "\n",
    ")\n",
    "p2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Visualize the output\n",
    "!{('head -n 20 data/outcreate1-00000-of-00001')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-24.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pubsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marco', 31)\n",
      "('Rebekah', 31)\n",
      "('Itoe', 31)\n",
      "('Edouard', 31)\n",
      "('Kyle', 62)\n",
      "('Kumiko', 31)\n",
      "('Gaston', 31)\n",
      "('Ayumi', 30)\n"
     ]
    }
   ],
   "source": [
    "p1 = beam.Pipeline()\n",
    "def SplitRow(element):\n",
    "    return element.split(',')\n",
    "\n",
    "attendance_count=(\n",
    "    p1\n",
    "     |beam.io.ReadFromText(\"dept_data.txt\")\n",
    "     |beam.Map(SplitRow) #takes one element as input and output\n",
    "     |beam.Filter(lambda record : record[3] == 'Accounts') \n",
    "     |beam.Map(lambda record : (record[1],1)) # append 1 to each record and get the name and 1 as two columns\n",
    "     |beam.CombinePerKey(sum)\n",
    "     |beam.io.WriteToText('data/output_new')\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "p1.run()\n",
    "!{('head -n 20 data/output_new-00000-of-00001')}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map vs Flatmap\n",
    "# Flat map 1 to Many mapping -> multiple output possible\n",
    "# Map only 1 to 1 mapping -> single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marco', 31)\n",
      "('Rebekah', 31)\n",
      "('Itoe', 31)\n",
      "('Edouard', 31)\n",
      "('Kyle', 62)\n",
      "('Kumiko', 31)\n",
      "('Gaston', 31)\n",
      "('Ayumi', 30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#labelling\n",
    "#other ways of running pipeline\n",
    "\n",
    "def SplitRow(element):\n",
    "    return element.split(',')\n",
    "\n",
    "with beam.Pipeline() as p1:\n",
    "    attendance_count=(\n",
    "        p1\n",
    "        |'Read from file' >> beam.io.ReadFromText(\"dept_data.txt\")\n",
    "        |'Map' >> beam.Map(SplitRow) #takes one element as input and output\n",
    "        |beam.Filter(lambda record : record[3] == 'Accounts') \n",
    "        |beam.Map(lambda record : (record[1],1)) # append 1 to each record and get the name and 1 as two columns\n",
    "        |beam.CombinePerKey(sum)\n",
    "        |beam.io.WriteToText('data/output_new')\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "#p1.run()\n",
    "!{('head -n 20 data/output_new-00000-of-00001')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching and flattening\n",
    "# flattens combines two pcollection if they have same datatype and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pardo is generic case. Map and Flatmap are special cases of pardo transform\n",
    "\n",
    "# Create a class inherating DoFn class. DoFn implements distributed processing\n",
    "# We need to overwrite only the process method.\n",
    "\n",
    "#can also use lambda function. beam internally creates dofn lightweight objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class SplitRow(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "    # return type -> list\n",
    "    return  [element.split(',')]\n",
    "  \n",
    "\n",
    "class FilterAccountsEmployee(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "    if element[3] == 'Accounts':\n",
    "      return [element]  \n",
    "    \n",
    "class PairEmployees(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "    return [(element[3]+\",\"+element[1], 1)]    \n",
    "  \n",
    "class Counting(beam.DoFn):\n",
    "  \n",
    "  def process(self, element):\n",
    "    # return type -> list\n",
    "    (key, values) = element           # [Marco, Accounts  [1,1,1,1....] , Rebekah, Accounts [1,1,1,1,....] ]\n",
    "    return [(key, sum(values))]\n",
    "     \n",
    "\n",
    "p1 = beam.Pipeline()\n",
    "\n",
    "attendance_count = (\n",
    "    \n",
    "   p1\n",
    "    |beam.io.ReadFromText('dept_data.txt')\n",
    "    \n",
    "    |beam.ParDo(SplitRow())\n",
    "   # | 'Compute WordLength' >> beam.ParDo(lambda element: [ element.split(',') ]) \n",
    "\n",
    "    |beam.ParDo(FilterAccountsEmployee())\n",
    "    |beam.ParDo(PairEmployees())\n",
    "    | 'Group ' >> beam.GroupByKey()\n",
    "    | 'Sum using ParDo' >> beam.ParDo(Counting())  \n",
    "    \n",
    "    |beam.io.WriteToText('data/output_new_final')\n",
    "  \n",
    ")\n",
    "\n",
    "p1.run()\n",
    "\n",
    "# Sample the first 20 results, remember there are no ordering guarantees.\n",
    "!{('head -n 20 data/output_new_final-00000-of-00001')}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
