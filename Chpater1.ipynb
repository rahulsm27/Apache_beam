{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch vs Stream\n",
    "# Here are some differences between batch processing and stream processing: \n",
    "\n",
    "# Data size\n",
    "# In batch processing, the data size is known and finite. In stream processing, the data size is unknown and infinite.\n",
    "\n",
    "# Data collection\n",
    "# In batch processing, data is collected over time and then processed all at once. In stream processing, data is continuously collected and processed in real time as it arrives.\n",
    "\n",
    "# Data processing\n",
    "# Batch processing handles a large batch of data. Stream processing handles individual records or micro batches of few records.\n",
    "\n",
    "# Use cases\n",
    "# Batch processing is generally meant for large data quantities that are not time sensitive. Stream processing is typically meant for data needed immediately. Stream processing applications can handle high volumes and velocities of data, and provide low-latency and timely insights. Stream processing is suitable for use cases such as fraud detection, anomaly detection, event processing, and real-time analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evolution of big data framework\n",
    "\n",
    "# MapReduce (2004): Google introduced the concept of MapReduce as a programming model for processing and generating large datasets on a distributed cluster. This inspired open-source implementations like Apache Hadoop.\n",
    "\n",
    "# Hadoop (2006): The Apache Hadoop project emerged as an open-source implementation of Google's MapReduce, providing a distributed file system (HDFS) and a framework for processing large-scale data across clusters. Hadoop played a pivotal role in making big data accessible to a broader audience.\n",
    "\n",
    "# Apache Spark (2010): Spark was introduced as an open-source, distributed computing system that aimed to improve upon the limitations of MapReduce. It introduced in-memory processing, which significantly accelerated iterative algorithms and machine learning workflows compared to Hadoop.\n",
    "\n",
    "# Apache Flink (2014): Flink is a stream processing framework that focuses on low-latency, high-throughput processing of continuous data streams. It's designed to handle both batch and stream processing, offering better performance and expressiveness for certain use cases compared to Spark.\n",
    "\n",
    "# Apache Kafka (2011): Kafka is a distributed streaming platform that enables the building of real-time data pipelines and streaming applications. It's widely used for its durability, fault-tolerance, and ability to handle high-throughput data streams.\n",
    "\n",
    "# Apache Storm (2011): Storm was one of the early stream processing frameworks, designed for real-time data processing. While it has been succeeded by other frameworks like Flink and Spark Streaming for some use cases, Storm played a role in shaping the landscape of real-time data processing.\n",
    "\n",
    "# Apache HBase (2007): HBase is a distributed, scalable, and consistent NoSQL database that runs on top of Hadoop's HDFS. It provides random access to large amounts of structured data and is often used for real-time read and write access to big data.\n",
    "\n",
    "# Apache Beam (2016): Beam is a unified stream and batch processing model designed to provide a portable, flexible, and extensible way to express data processing pipelines. It supports multiple backends, including Apache Spark and Apache Flink.\n",
    "\n",
    "# Dask (2015): Dask is a parallel computing library that integrates with Python and allows users to harness the full power of their CPU and memory resources for analytics. While not exclusively a big data framework, it is often used for scalable data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Architecture\n",
    "# Apache Beam is an open-source, unified model for defining both batch and streaming data-parallel processing pipelines. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Apex, Apache Flink, Apache Spark, and Google Cloud Dataflow. —"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fow of beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
